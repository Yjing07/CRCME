{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CRCME Feature Extraction Demo\n",
    "\n",
    "This notebook demonstrates how to:\n",
    "1. Load the CRCME pre-trained model (CT + joint/FU weights)\n",
    "2. Encode a single CT image to extract global features\n",
    "3. Optionally, batch encode multiple images from a folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Install and Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# !pip install torch torchvision SimpleITK numpy matplotlib\n",
    "\n",
    "import torch\n",
    "import os\n",
    "import sys\n",
    "current_dir = os.getcwd()  # 替代 __file__\n",
    "root_dir = os.path.abspath(os.path.join(current_dir, \"..\"))\n",
    "sys.path.append(root_dir)\n",
    "import SimpleITK as sitk\n",
    "import numpy as np\n",
    "from lib.model_MOE import ViT_ct, ViT_fu, FusionModel, FusionPipeline\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Initialize Models and Load Pretrained Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained weights from: /cache/yangjing/main_files/CRCFound2/argo2/mymodel/checkpoint-999.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1249059/520511555.py:32: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(path, map_location='cpu')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrained weights loaded successfully.\n",
      "Loading pretrained weights from: /cache/yangjing/main_files/CRCFound2/CRCFound1/logs_1w/mix_no_ada_2k/patch32_frame32_large_256-None/logs/checkpoint-300.pth\n",
      "Pretrained weights loaded successfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "FusionPipeline(\n",
       "  (model_a): ViT_ct(\n",
       "    (patch_embed): PatchEmbed(\n",
       "      (proj): Conv3d(1, 1024, kernel_size=(16, 16, 16), stride=(16, 16, 16))\n",
       "    )\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (blocks): ModuleList(\n",
       "      (0-23): 24 x Block(\n",
       "        (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    (to_latent): Identity()\n",
       "    (mlp_head): Sequential(\n",
       "      (0): Linear(in_features=1024, out_features=4, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (model_b): ViT_fu(\n",
       "    (patch_embed): PatchEmbed(\n",
       "      (proj): Conv3d(1, 1024, kernel_size=(16, 16, 16), stride=(16, 16, 16))\n",
       "    )\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "    (blocks): ModuleList(\n",
       "      (0-23): 24 x Block(\n",
       "        (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    (to_latent): Identity()\n",
       "    (mlp_head): Sequential(\n",
       "      (0): Linear(in_features=1024, out_features=4, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (fusion): FusionModel(\n",
       "    (fc1): Linear(in_features=2048, out_features=512, bias=True)\n",
       "    (weight_generator): Sequential(\n",
       "      (0): Linear(in_features=2048, out_features=1024, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=1024, out_features=2, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (weight_net): WeightNet(\n",
       "    (fc1): Linear(in_features=2048, out_features=32, bias=True)\n",
       "    (fc2): Linear(in_features=32, out_features=2, bias=True)\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Flatten(start_dim=1, end_dim=-1)\n",
       "    (1): Linear(in_features=1024, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize models\n",
    "model_a = ViT_ct(\n",
    "    image_size = 256,\n",
    "    frames = 32,\n",
    "    image_patch_size = 16,\n",
    "    frame_patch_size = 16,\n",
    "    dim = 1024,\n",
    "    depth = 24,\n",
    "    heads = 16,\n",
    "    emb_dropout = 0.1)\n",
    "\n",
    "model_b = ViT_fu(\n",
    "    image_size = 256,\n",
    "    frames = 32,\n",
    "    image_patch_size = 16,\n",
    "    frame_patch_size = 16,\n",
    "    dim = 1024,\n",
    "    depth = 24,\n",
    "    heads = 16)\n",
    "\n",
    "fusion_model = FusionModel(input_dim_a=1024, input_dim_b=1024, classes=2)  # TODO: modify num_classes\n",
    "pipeline = FusionPipeline(model_a, model_b, fusion_model, num_classes=2)\n",
    "\n",
    "# Paths to pretrained weights\n",
    "pretrained_ct = 'checkpoints/checkpoint-ct.pth'        # TODO: modify path\n",
    "pretrained_fu = 'checkpoints/checkpoint-joint.pth'     # TODO: modify path\n",
    "\n",
    "# Helper function to load weights\n",
    "def load_weights(model, path):\n",
    "    if os.path.isfile(path):\n",
    "        print(f\"Loading pretrained weights from: {path}\")\n",
    "        checkpoint = torch.load(path, map_location='cpu')\n",
    "        pretrained_dict = checkpoint.get('model', checkpoint)\n",
    "        cleaned_dict = {k.replace(\"module.\", \"\"): v for k, v in pretrained_dict.items()}\n",
    "        compatible_dict = {k: v for k, v in cleaned_dict.items() if k in model.state_dict()}\n",
    "        model.load_state_dict({**model.state_dict(), **compatible_dict})\n",
    "        print(\"Pretrained weights loaded successfully.\")\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"Pretrained model not found at: {path}\")\n",
    "\n",
    "# Load weights\n",
    "load_weights(model_a, pretrained_ct)\n",
    "load_weights(model_b, pretrained_fu)\n",
    "\n",
    "# Move pipeline to device\n",
    "pipeline.to(device)\n",
    "pipeline.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Encode a Single Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted feature shape: (1, 1024)\n"
     ]
    }
   ],
   "source": [
    "# Load a CT image\n",
    "img_path = 'path/to/ct_images/'  # TODO: modify path\n",
    "img = sitk.ReadImage(img_path)\n",
    "img_array = sitk.GetArrayFromImage(img).astype(np.float32)\n",
    "\n",
    "# Convert to tensor and add batch & channel dims\n",
    "img_tensor = torch.from_numpy(img_array).unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "# Extract image features\n",
    "with torch.inference_mode():\n",
    "    _,image_embeddings,_ = pipeline(\n",
    "        img_tensor.to(device),\n",
    "        flag=1   # return global feature token\n",
    "    )\n",
    "print(\"Extracted feature shape:\", image_embeddings.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. (Optional) Batch Encode a Folder of CT Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "img_folder = 'path/to/ct_images/'  # TODO: modify folder path\n",
    "img_paths = sorted(glob.glob(os.path.join(img_folder, '*.nii')))  # adjust extension if needed\n",
    "\n",
    "features_list = []\n",
    "\n",
    "for p in img_paths:\n",
    "    img = sitk.ReadImage(p)\n",
    "    img_array = sitk.GetArrayFromImage(img).astype(np.float32)\n",
    "    img_tensor = torch.from_numpy(img_array).unsqueeze(0).unsqueeze(0)\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        feat = pipeline(img_tensor.to(device), return_features=True)\n",
    "    features_list.append(feat.cpu().numpy())\n",
    "\n",
    "print(f\"Extracted features for {len(features_list)} images, each shape: {features_list[0].shape}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
